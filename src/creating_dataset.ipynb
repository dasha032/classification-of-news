{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38be307b",
   "metadata": {},
   "source": [
    "# Работа с датасетами "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a645a5f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "# Загрузка датасетов\n",
    "real_news_full = load_dataset('IlyaGusev/ru_news', split=\"train\", streaming=True, trust_remote_code=True)\n",
    "fake_news_full = load_dataset(\"its5Q/panorama\", split=\"train\", streaming=True, trust_remote_code=True)\n",
    "\n",
    "real_news = real_news_full.take(10000)\n",
    "fake_news = fake_news_full.take(10000)\n",
    "\n",
    "# Добавление меток\n",
    "real_news = real_news.map(lambda x: {'text': x['text'], \"label\": 1}, remove_columns=[col for col in real_news.features if col != \"text\"])\n",
    "fake_news = fake_news.map(lambda x: {'text': x['body'], \"label\": 0}, remove_columns=[col for col in fake_news.features if col != \"text\"])\n",
    "\n",
    "# Объединение датасетов\n",
    "combined_dataset = concatenate_datasets([real_news, fake_news])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073d538d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(combined_dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9c056",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('all_news_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71436a1b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "df = pd.read_csv('all_news_data.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e8735",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds \n",
    "\n",
    "# Создание набора данных\n",
    "target = df.pop('label')\n",
    "df['text'] = df['text'].astype(str)  \n",
    "\n",
    "ds_raw = tf.data.Dataset.from_tensor_slices((df.values, target.values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf02293",
   "metadata": {},
   "source": [
    "Разделение данных на выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bad7f9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "ds_raw = ds_raw.shuffle(20000, reshuffle_each_iteration=False)\n",
    "\n",
    "ds_raw_test = ds_raw.take(10000)\n",
    "ds_raw_train_valid = ds_raw.skip(10000)\n",
    "ds_raw_train = ds_raw_train_valid.take(6000)\n",
    "ds_raw_valid = ds_raw_train_valid.skip(6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e1158",
   "metadata": {},
   "source": [
    "# Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5e0b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2 \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = (stopwords.words('russian'))+ ['который', 'это', 'наш', 'свой', 'также', 'всё', 'весь']  \n",
    "stop_words = {word.lower() for word in stop_words}\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "all_text = \"\"\n",
    "\n",
    "for example in ds_raw_train:\n",
    "    text = example[0].numpy()[0].decode('utf-8') \n",
    "    all_text += \" \" + text  \n",
    "\n",
    "tokens = wordpunct_tokenize(all_text.lower())  \n",
    "lemmatized_tokens = []\n",
    "for word in tokens:\n",
    "    if word.isalpha():  \n",
    "        lemma = morph.parse(word)[0].normal_form  \n",
    "        if lemma not in stop_words:  \n",
    "            lemmatized_tokens.append(lemma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4dba2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "all_texts = [example[0].numpy()[0].decode('utf-8') for example in ds_raw_train]  \n",
    "\n",
    "\n",
    "VOCAB_SIZE = 30000  \n",
    "OOV_TOKEN = \"<UNK>\" \n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=VOCAB_SIZE,\n",
    "    oov_token=OOV_TOKEN,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'  \n",
    ")\n",
    "tokenizer.fit_on_texts(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b8e11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "    text = text_tensor.numpy()[0].decode('utf-8')  # Декодируем текст\n",
    "    encoded_text = tokenizer.texts_to_sequences([text])[0]  # Преобразуем в индексы\n",
    "    return encoded_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59a256",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def encode_map_fn(text, label):\n",
    "    return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb992d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Кодирование наборов в целые числа\n",
    "ds_train = ds_raw_train.map(encode_map_fn)\n",
    "ds_valid = ds_raw_valid.map(encode_map_fn)\n",
    "ds_test = ds_raw_test.map(encode_map_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d16ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Деление всех трех наборов данных на мини-пакеты с размером пакета 32\n",
    "train_data = ds_train.padded_batch(32, padded_shapes=([-1], []))\n",
    "valid_data = ds_valid.padded_batch(32, padded_shapes=([-1], []))\n",
    "test_data = ds_test.padded_batch(32, padded_shapes=([-1], []))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5533bad4",
   "metadata": {},
   "source": [
    "# Построение модели на основе RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27593e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 20\n",
    "vocab_size = len(token_counts) + 2\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "bi_lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(32,)),\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        name='embed-layer'\n",
    "    ),\n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(64, name='lstm-layer'),\n",
    "        name='bidir-lstm'\n",
    "    ),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "bi_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc9df6a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Компиляция и обучение\n",
    "bi_lstm_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = bi_lstm_model.fit(train_data,validation_data=valid_data,\n",
    "                            epochs=7)\n",
    "# Оценка на тестовых данных\n",
    "test_results = bi_lstm_model.evaluate(test_data)\n",
    "print('Результат на тестовых: {:.2f}%'.format(test_results[1]*100))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
